- ### Must read papers
    **1) RNN**
    - RNN : Recurrent neural network based language model (2010)
    - LSTM : Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling (2014)
    - GRU : Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation (2014)
    - Seq2Seq : Sequence to Sequence Learning with Neural Networks (2014)
    
    **2) Attention Mechanism**
    - Attention : Neural Machine Translation by Jointly Learning to Align and Translate (2015)
    - Transformer : Attention is All You Need (2017)
    
    **3) Word Embedding**
    - Word2Vec : Efficient Estimation of Word Representations in Vector Space (2013)
    - GloVe : Global Vectors for Word Representation (2014)
    - FastText : Enriching Word Vectors with Subword Information (2016)
    - ELMo : Deep contextualized word representations (2018)
    
    **4) Transformer Architecture Based Models**
    - GPT-1 : Improving Language Understanding by Generative Pre-Training (2018)
    - BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
    - GPT-2 : Language Models are Unsupervised Multitask Learners (2018)
    - RoBERTa : A Robustly Optimized BERT Pretraining Approach (2019)
    - ALBERT : A Lite BERT for Self-supervised Learning of Language Representations (2019)
    - ELECTRA : Pre-training Text Encoders as Discriminators Rather Than Generators (2020)
    - XLNet : Generalized Autoregressive Pretraining for Language Understanding (2019)
    - T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019)
    
    **5) Good papers to read**
    - Recursive deep models for semantic compositionality over a sentiment treebank(2013)
    - Distributed representations of words and phrases and their compositionality (2013)
    - Distributed representations of sentences and documents (2014)
    - Memory networks (2014)
    - Convolutional neural networks for sentence classification (2014)
    - Neural machine translation by jointly learning to align and translate (2014)
    - Effective approaches to attention-based neural machine translation (2015)
    - Neural Architectures for Named Entity Recognition (2016),
    - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (2017)
    - Overcoming catastrophic forgetting in neural networks (2017)
    - A Simple Framework for Contrastive Learning of Visual Representations (2020)
    - Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks (2020)
